{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# First Draft Submission for test for Symbolic Representations of Probability Amplitude for High Energy Physics (Parv Agarwal)\n",
        "\n",
        "This submission is more a proof of concept than an actual submission, looking for feedback on the method before moving forward with improving the accuracy and ability of the method. \n",
        "\n",
        "Currently, we generate ranges of function combinations using the exponential and trigonometric functions and then encode them into a dataset using character encoding and then train a LSTM model on it. Since it's in the preliminary nature, not much attention has been paid to the accuracy of the model or the nature of the dataset, since the level of demonstration required is unclear."
      ],
      "metadata": {
        "id": "Mni_2R3kOlw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verison of Events\n",
        "\n",
        "I am not an experienced campaigner when it comes to sequence to sequence learning. To get my hands dirty, I first reproduced this tutorial on language translation from - https://analyticsindiamag.com/sequence-to-sequence-modeling-using-lstm-for-language-translation/ in order to get a handle of it. A lot of the code here is derived from there. I adapted the code for this specific use case."
      ],
      "metadata": {
        "id": "Zt4E0O82Q4fh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XR4Thh-5E4Yp"
      },
      "outputs": [],
      "source": [
        "from sympy import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we must generate the dataset of the functions with their Taylor Expansions. In order to do that, we first create the Taylor method that expands the function to the fourth order based on the Taylor Expansion rules. We then create combinations of functions using the common functions - Exponential and the Trigonometric cos, sin and tan. For a better model, we would generally require a larger dataset that is more representative of all the types of functions present, however, this is more of a proof of concept. "
      ],
      "metadata": {
        "id": "_idD9R9fFw9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = symbols(\"x\")\n",
        "\n",
        "\"\"\" Generating the dataset \"\"\"\n",
        "\n",
        "# Taylor approximation at x0 of the function 'function'\n",
        "def taylor(function,x0,n):\n",
        "    i = 0\n",
        "    p = 0\n",
        "    while i <= n:\n",
        "        p = Add(p, (function.diff(x,i).subs(x,x0))/(factorial(i))*(x-x0)**i) # subs evaluates the function\n",
        "        i += 1\n",
        "    return p\n",
        "\n",
        "\n",
        "function_operators = [cos(x), sin(x), exp(x), tan(x)]\n",
        "# Create ranges of expressions using the three operators to Taylor expand and create dataset\n",
        "\n",
        "function_dataset = []\n",
        "\n",
        "for pow in range(0, 6):\n",
        "    for i in range(0, len(function_operators)):\n",
        "        for j in range(0, len(function_operators)):\n",
        "            function_dataset.append(function_operators[i]**pow * function_operators[j])\n",
        "            function_dataset.append(function_operators[i] * function_operators[j]**pow)\n",
        "\n",
        "\n",
        "\n",
        "taylor_rep = [taylor(i,0,4) for i in function_dataset]\n",
        "\n",
        "for i in range(0,10):\n",
        "  print(f\"{function_dataset[i]} - {taylor_rep[i]}\")"
      ],
      "metadata": {
        "id": "1OAbobpIE_DO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b0aec2-7300-4375-8aca-c2f6bd1de832"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cos(x) - x**4/24 - x**2/2 + 1\n",
            "cos(x) - x**4/24 - x**2/2 + 1\n",
            "sin(x) - -x**3/6 + x\n",
            "cos(x) - x**4/24 - x**2/2 + 1\n",
            "exp(x) - x**4/24 + x**3/6 + x**2/2 + x + 1\n",
            "cos(x) - x**4/24 - x**2/2 + 1\n",
            "tan(x) - x**3/3 + x\n",
            "cos(x) - x**4/24 - x**2/2 + 1\n",
            "cos(x) - x**4/24 - x**2/2 + 1\n",
            "sin(x) - -x**3/6 + x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start preparing the preprocessing for training a LSTM model. We seperate the input and target sequences based on the Functions and their Taylor representations and prepare the number of tokens for input and target sequences."
      ],
      "metadata": {
        "id": "qrmTBlWoGRh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.utils import *\n",
        "from keras.initializers import *\n",
        "import tensorflow as tf\n",
        "import time, random\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "\n",
        "# Vectorizing data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_chars = set()\n",
        "target_chars = set()"
      ],
      "metadata": {
        "id": "fucACSQAFLEO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(0, len(function_dataset)):\n",
        "  input_text, target_text = str(function_dataset[i]), str(taylor_rep[i])  \n",
        "  target_text = '\\t' + target_text + '\\n'\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  for char in input_text:\n",
        "      if char not in input_chars:\n",
        "          input_chars.add(char)\n",
        "  for char in target_text:\n",
        "      if char not in target_chars:\n",
        "          target_chars.add(char)\n",
        "\n",
        "input_chars = sorted(list(input_chars))\n",
        "target_chars = sorted(list(target_chars))\n",
        "num_encoder_tokens = len(input_chars)\n",
        "num_decoder_tokens = len(target_chars)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "#Print size\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y1j7WgXFSad",
        "outputId": "31091435-8519-4825-f40a-8b03bbcc7e2f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 192\n",
            "Number of unique input tokens: 18\n",
            "Number of unique output tokens: 18\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create the data specific for the encoder and decoder, and prepare the encoder and decoder and the Model"
      ],
      "metadata": {
        "id": "Ca8xGHS4GoOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define data for encoder and decoder\n",
        "input_token_id = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_id = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "\n",
        "encoder_in_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "\n",
        "decoder_in_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_in_data[i, t, input_token_id[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_in_data[i, t, target_token_id[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_id[char]] = 1.\n",
        "    \n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Using encoder states to set up the deecoder as initial state\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Final Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()\n",
        "\n",
        "#Model data Shape\n",
        "print(\"encoder_in_data shape:\",encoder_in_data.shape)\n",
        "print(\"decoder_in_data shape:\",decoder_in_data.shape)\n",
        "print(\"decoder_target_data shape:\",decoder_target_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9qG5u3KFWlm",
        "outputId": "67183dda-ccba-474e-fd85-026b0fc416d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, None, 18)]   0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, None, 18)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 256),        281600      ['input_5[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  281600      ['input_6[0][0]',                \n",
            "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 18)     4626        ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 567,826\n",
            "Trainable params: 567,826\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "encoder_in_data shape: (192, 16, 18)\n",
            "decoder_in_data shape: (192, 48, 18)\n",
            "decoder_target_data shape: (192, 48, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now train the model and test it on a finite number of sequences."
      ],
      "metadata": {
        "id": "18zsm8GNGykW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.01, beta_1=0.9, beta_2 = 0.999, decay=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([encoder_in_data, decoder_in_data], decoder_target_data, batch_size=batch_size, epochs=50, validation_split=0.2)\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_id.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_id.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    #Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    #Get the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_id['\\t']] = 1.\n",
        "\n",
        "    #Sampling loop for a batch of sequences\n",
        "    #(to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        #Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        #Exit condition: either hit max length\n",
        "        #or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "        len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        #Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        #Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "\n",
        "for seq_index in range(10):\n",
        "    input_seq = encoder_in_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsQ7-K2yFZa1",
        "outputId": "bdf3e42e-fa87-4cae-87b5-8d2e02662da3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - 7s 849ms/step - loss: 1.1870 - accuracy: 0.0915 - val_loss: 0.8627 - val_accuracy: 0.0556\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 1s 276ms/step - loss: 1.0487 - accuracy: 0.0738 - val_loss: 0.8091 - val_accuracy: 0.0406\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 1s 298ms/step - loss: 0.9739 - accuracy: 0.0881 - val_loss: 0.7861 - val_accuracy: 0.0705\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 2s 699ms/step - loss: 0.9265 - accuracy: 0.1129 - val_loss: 0.7563 - val_accuracy: 0.0817\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 1s 446ms/step - loss: 0.8686 - accuracy: 0.1310 - val_loss: 0.7466 - val_accuracy: 0.0817\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 1s 421ms/step - loss: 0.8459 - accuracy: 0.1300 - val_loss: 0.7418 - val_accuracy: 0.0710\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 1s 247ms/step - loss: 0.8484 - accuracy: 0.1077 - val_loss: 0.7370 - val_accuracy: 0.0710\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 1s 229ms/step - loss: 0.8571 - accuracy: 0.1063 - val_loss: 0.7372 - val_accuracy: 0.0780\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 1s 341ms/step - loss: 0.8731 - accuracy: 0.1147 - val_loss: 0.7204 - val_accuracy: 0.0678\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 1s 243ms/step - loss: 0.8579 - accuracy: 0.0990 - val_loss: 0.7429 - val_accuracy: 0.0807\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 1s 435ms/step - loss: 0.8402 - accuracy: 0.1265 - val_loss: 0.7145 - val_accuracy: 0.0726\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 1s 236ms/step - loss: 0.8383 - accuracy: 0.1230 - val_loss: 0.7309 - val_accuracy: 0.0710\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 1s 248ms/step - loss: 0.8617 - accuracy: 0.1083 - val_loss: 0.7231 - val_accuracy: 0.0710\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 1s 237ms/step - loss: 0.8372 - accuracy: 0.1083 - val_loss: 0.7157 - val_accuracy: 0.0710\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 1s 284ms/step - loss: 0.8226 - accuracy: 0.1201 - val_loss: 0.7262 - val_accuracy: 0.0817\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 1s 267ms/step - loss: 0.8154 - accuracy: 0.1315 - val_loss: 0.7204 - val_accuracy: 0.0796\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.8159 - accuracy: 0.1228 - val_loss: 0.6964 - val_accuracy: 0.0780\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 1s 416ms/step - loss: 0.8036 - accuracy: 0.1228 - val_loss: 0.7027 - val_accuracy: 0.0705\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 1s 236ms/step - loss: 0.8018 - accuracy: 0.1295 - val_loss: 0.6831 - val_accuracy: 0.0865\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 1s 251ms/step - loss: 0.7941 - accuracy: 0.1480 - val_loss: 0.6881 - val_accuracy: 0.0839\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 1s 281ms/step - loss: 0.7835 - accuracy: 0.1446 - val_loss: 0.6941 - val_accuracy: 0.0694\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 1s 242ms/step - loss: 0.7808 - accuracy: 0.1456 - val_loss: 0.6798 - val_accuracy: 0.0849\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 1s 239ms/step - loss: 0.7783 - accuracy: 0.1319 - val_loss: 0.6901 - val_accuracy: 0.0764\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 1s 290ms/step - loss: 0.7720 - accuracy: 0.1520 - val_loss: 0.6752 - val_accuracy: 0.0913\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 1s 283ms/step - loss: 0.7670 - accuracy: 0.1411 - val_loss: 0.6855 - val_accuracy: 0.0833\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 1s 235ms/step - loss: 0.7689 - accuracy: 0.1426 - val_loss: 0.6726 - val_accuracy: 0.0876\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 1s 275ms/step - loss: 0.7632 - accuracy: 0.1423 - val_loss: 0.6804 - val_accuracy: 0.0839\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 1s 243ms/step - loss: 0.7570 - accuracy: 0.1552 - val_loss: 0.6665 - val_accuracy: 0.0849\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 1s 293ms/step - loss: 0.7566 - accuracy: 0.1566 - val_loss: 0.6711 - val_accuracy: 0.0823\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 1s 242ms/step - loss: 0.7521 - accuracy: 0.1407 - val_loss: 0.6589 - val_accuracy: 0.0935\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 1s 235ms/step - loss: 0.7527 - accuracy: 0.1509 - val_loss: 0.6747 - val_accuracy: 0.0828\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 1s 401ms/step - loss: 0.7465 - accuracy: 0.1635 - val_loss: 0.6571 - val_accuracy: 0.0865\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 1s 381ms/step - loss: 0.7576 - accuracy: 0.1615 - val_loss: 0.6520 - val_accuracy: 0.0924\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 1s 272ms/step - loss: 0.7499 - accuracy: 0.1481 - val_loss: 0.6618 - val_accuracy: 0.0775\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 1s 241ms/step - loss: 0.7674 - accuracy: 0.1337 - val_loss: 0.6736 - val_accuracy: 0.0769\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 1s 247ms/step - loss: 0.7460 - accuracy: 0.1494 - val_loss: 0.6544 - val_accuracy: 0.0881\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 1s 242ms/step - loss: 0.7514 - accuracy: 0.1584 - val_loss: 0.6490 - val_accuracy: 0.0887\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 1s 235ms/step - loss: 0.7532 - accuracy: 0.1570 - val_loss: 0.6537 - val_accuracy: 0.0908\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 1s 244ms/step - loss: 0.7480 - accuracy: 0.1467 - val_loss: 0.6358 - val_accuracy: 0.0951\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 1s 239ms/step - loss: 0.7439 - accuracy: 0.1520 - val_loss: 0.6248 - val_accuracy: 0.0988\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 1s 279ms/step - loss: 0.7402 - accuracy: 0.1642 - val_loss: 0.6297 - val_accuracy: 0.0983\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 1s 249ms/step - loss: 0.7429 - accuracy: 0.1452 - val_loss: 0.6290 - val_accuracy: 0.0983\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 1s 245ms/step - loss: 0.7378 - accuracy: 0.1531 - val_loss: 0.6253 - val_accuracy: 0.1052\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 1s 237ms/step - loss: 0.7331 - accuracy: 0.1673 - val_loss: 0.6340 - val_accuracy: 0.1036\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 1s 243ms/step - loss: 0.7447 - accuracy: 0.1545 - val_loss: 0.6490 - val_accuracy: 0.0903\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 1s 276ms/step - loss: 0.7338 - accuracy: 0.1522 - val_loss: 0.6248 - val_accuracy: 0.1036\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 1s 282ms/step - loss: 0.7389 - accuracy: 0.1570 - val_loss: 0.6158 - val_accuracy: 0.1052\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.7296 - accuracy: 0.1626 - val_loss: 0.6227 - val_accuracy: 0.1031\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 1s 387ms/step - loss: 0.7269 - accuracy: 0.1509 - val_loss: 0.6263 - val_accuracy: 0.1015\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 1s 232ms/step - loss: 0.7286 - accuracy: 0.1536 - val_loss: 0.6091 - val_accuracy: 0.1084\n",
            "1/1 [==============================] - 0s 392ms/step\n",
            "1/1 [==============================] - 0s 422ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: cos(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "-\n",
            "Input sentence: cos(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "-\n",
            "Input sentence: sin(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: cos(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "-\n",
            "Input sentence: exp(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "-\n",
            "Input sentence: cos(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "-\n",
            "Input sentence: tan(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "-\n",
            "Input sentence: cos(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "-\n",
            "Input sentence: cos(x)\n",
            "Decoded sentence: x**4/                                            \n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "-\n",
            "Input sentence: sin(x)\n",
            "Decoded sentence: x**4/                                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the model performs pretty poorly. The following steps can significanlty improve it's performance\n",
        "\n",
        "1. Increasing size and range of dataset - Currently it has singular permuations of the main functions - cosx, sinx, tanx, and the exponential. By permuting over every combination and to a power of 6, we can create a much larger and improved dataset with a size of upto a thousand entries. This will give our model a lot more time to fit to the data and learn the patterns. \n",
        "\n",
        "2. Using a different encoding strategy for Tokenisation - Currently a simple character encoding is used to encode the dataset to numerical form. It has been found often that specific types of encoding yield better results (for instance Byte Pair Encoding) for certain types of sequence to sequence learning tasks. \n",
        "\n",
        "3. Changing model parameters - This is a more obvious one, but adapting the model for the specific use case would yield much better results than attempting to use it straight out of the gate. \n",
        "\n"
      ],
      "metadata": {
        "id": "vSVQs30bPvrI"
      }
    }
  ]
}